test_cases:
  - id: "sdp_bronze_ingestion_001"
    inputs:
      prompt: "Create a bronze ingestion pipeline for JSON files in /Volumes/raw/orders"
    outputs:
      response: |
        Here's a bronze ingestion pipeline using Spark Declarative Pipelines:

        ```sql
        CREATE OR REFRESH STREAMING TABLE bronze_orders
        CLUSTER BY (order_date)
        AS SELECT
          *,
          current_timestamp() AS _ingested_at,
          input_file_name() AS _source_file
        FROM read_files(
          '/Volumes/raw/orders',
          format => 'json',
          inferSchema => true
        );
        ```

        This pipeline:
        - Uses STREAMING TABLE for incremental ingestion
        - Uses CLUSTER BY instead of PARTITION BY for better performance
        - Adds metadata columns for lineage tracking
        - Uses read_files() for Auto Loader functionality
      execution_success: true
    expectations:
      expected_facts:
        - "STREAMING TABLE"
        - "CLUSTER BY"
        - "_ingested_at"
      expected_patterns:
        - pattern: "CREATE OR REFRESH STREAMING TABLE"
          min_count: 1
          description: "CREATE OR REFRESH STREAMING"
        - pattern: "CLUSTER BY"
          min_count: 1
          description: "CLUSTER BY"
        - pattern: "read_files\\s*\\("
          min_count: 1
          description: "read_files"
      guidelines:
        - "Must use modern SDP syntax, not legacy DLT"
        - "Should include metadata columns for lineage"
    metadata:
      category: "happy_path"
      difficulty: "easy"
      source: "manual"
      tags: ["bronze", "ingestion", "autoloader"]

  - id: "sdp_cdc_scd2_sql_001"
    inputs:
      prompt: "Implement auto_cdc with SCD Type 2 for tracking customer dimension changes"
    outputs:
      response: |
        Here's an AUTO CDC implementation with SCD Type 2 using SQL:

        ```sql
        -- Bronze: Ingest raw CDC events from source system
        CREATE OR REFRESH STREAMING TABLE bronze_customers_cdc
        CLUSTER BY (customer_id)
        AS SELECT
          *,
          current_timestamp() AS _ingested_at,
          _metadata.file_path AS _source_file
        FROM STREAM read_files(
          '/Volumes/raw/customers_cdc',
          format => 'json',
          schemaHints => 'customer_id STRING, customer_name STRING, email STRING, phone STRING, address STRING, event_timestamp TIMESTAMP, operation STRING'
        );

        -- Silver: Apply SCD Type 2 using AUTO CDC
        CREATE OR REFRESH STREAMING TABLE silver_customers_history
        CLUSTER BY (customer_id);

        CREATE FLOW customers_scd2_flow AS
        AUTO CDC INTO silver_customers_history
        FROM STREAM bronze_customers_cdc
        KEYS (customer_id)
        SEQUENCE BY event_timestamp
        APPLY AS DELETE WHEN operation = 'DELETE'
        COLUMNS * EXCEPT (operation, _ingested_at, _source_file)
        STORED AS SCD TYPE 2
        TRACK HISTORY ON *;
        ```

        This pipeline:
        - Uses `AUTO CDC INTO` for automatic change data capture processing
        - Creates SCD Type 2 with automatic `__START_AT` and `__END_AT` columns
        - Handles INSERT, UPDATE, and DELETE operations from source
        - Uses `SEQUENCE BY` to handle out-of-order events correctly
        - Tracks history on all columns (changes create new versions)

        Query current state: `SELECT * FROM silver_customers_history WHERE __END_AT IS NULL`
      execution_success: null
    expectations:
      expected_facts:
        - "AUTO CDC"
        - "SCD TYPE 2"
        - "TRACK HISTORY"
        - "__END_AT"
      expected_patterns:
        - pattern: "CREATE OR REFRESH STREAMING TABLE"
          min_count: 2
          description: "streaming tables for bronze and silver"
        - pattern: "AUTO CDC INTO"
          min_count: 1
          description: "AUTO CDC syntax"
        - pattern: "STORED AS SCD TYPE 2"
          min_count: 1
          description: "SCD Type 2 specification"
        - pattern: "SEQUENCE BY"
          min_count: 1
          description: "ordering for out-of-order handling"
        - pattern: "TRACK HISTORY ON"
          min_count: 1
          description: "history tracking"
      guidelines:
        - "Must use AUTO CDC INTO syntax, not manual window functions"
        - "Must use STORED AS SCD TYPE 2"
        - "Should use SEQUENCE BY for ordering"
        - "Should handle DELETE operations"
    metadata:
      category: "happy_path"
      difficulty: "hard"
      source: "interactive"
      tags: ["cdc", "scd2", "sql", "auto_cdc", "dimensions"]

  - id: "sdp_silver_transform_001"
    inputs:
      prompt: "Create a silver transformation that cleans and deduplicates order data"
    outputs:
      response: |
        Here's a silver transformation for order data:

        ```sql
        CREATE OR REFRESH STREAMING TABLE silver_orders
        CLUSTER BY (order_date, customer_id)
        TBLPROPERTIES (
          'quality' = 'silver'
        )
        AS SELECT
          order_id,
          customer_id,
          order_date,
          TRIM(UPPER(status)) AS status,
          CAST(amount AS DECIMAL(10, 2)) AS amount,
          COALESCE(shipping_address, billing_address) AS address,
          _ingested_at,
          current_timestamp() AS _processed_at
        FROM (
          SELECT
            *,
            ROW_NUMBER() OVER (
              PARTITION BY order_id
              ORDER BY _ingested_at DESC
            ) AS _row_num
          FROM bronze_orders
          WHERE order_id IS NOT NULL
            AND amount > 0
        )
        WHERE _row_num = 1;
        ```

        This transformation:
        - Deduplicates using ROW_NUMBER window function
        - Standardizes string fields (TRIM, UPPER)
        - Casts amounts to proper decimal type
        - Handles null shipping addresses with COALESCE
        - Filters invalid records (null order_id, non-positive amounts)
      execution_success: true
    expectations:
      expected_facts:
        - "deduplicate"
        - "ROW_NUMBER"
        - "DECIMAL"
      expected_patterns:
        - pattern: "CREATE OR REFRESH STREAMING TABLE"
          min_count: 1
          description: "streaming table"
        - pattern: "ROW_NUMBER\\s*\\(\\)\\s*OVER"
          min_count: 1
          description: "deduplication"
        - pattern: "CLUSTER BY"
          min_count: 1
          description: "clustering"
      guidelines:
        - "Must deduplicate records properly"
        - "Should standardize data types and formats"
    metadata:
      category: "happy_path"
      difficulty: "medium"
      source: "manual"
      tags: ["silver", "transformation", "deduplication"]

  - id: "sdp_gold_aggregate_001"
    inputs:
      prompt: "Create a gold aggregate table for daily sales metrics"
    outputs:
      response: |
        Here's a gold aggregate for daily sales metrics:

        ```sql
        CREATE OR REFRESH MATERIALIZED VIEW gold_daily_sales
        CLUSTER BY (sale_date)
        AS SELECT
          DATE(order_date) AS sale_date,
          COUNT(DISTINCT order_id) AS total_orders,
          COUNT(DISTINCT customer_id) AS unique_customers,
          SUM(amount) AS total_revenue,
          AVG(amount) AS avg_order_value,
          MAX(amount) AS max_order_value,
          MIN(amount) AS min_order_value
        FROM silver_orders
        WHERE status = 'COMPLETED'
        GROUP BY DATE(order_date);
        ```

        This gold table:
        - Aggregates by date for time-series analysis
        - Calculates key business metrics
        - Uses MATERIALIZED VIEW for pre-computed results
        - Filters to completed orders only
      execution_success: true
    expectations:
      expected_facts:
        - "MATERIALIZED VIEW"
        - "GROUP BY"
        - "SUM"
      expected_patterns:
        - pattern: "CREATE OR REFRESH MATERIALIZED VIEW"
          min_count: 1
          description: "materialized view"
        - pattern: "GROUP BY"
          min_count: 1
          description: "aggregation"
        - pattern: "CLUSTER BY"
          min_count: 1
          description: "clustering"
      guidelines:
        - "Gold tables should be business-ready aggregates"
        - "Use MATERIALIZED VIEW for pre-computed aggregations"
    metadata:
      category: "happy_path"
      difficulty: "medium"
      source: "manual"
      tags: ["gold", "aggregation", "metrics"]

  - id: "sdp_streaming_windowed_001"
    inputs:
      prompt: "Create a streaming pipeline for IoT sensor data with 5-minute tumbling window aggregations"
    outputs:
      response: |
        Here's a streaming pipeline for IoT sensor data with windowed aggregations:

        ```sql
        -- Bronze: Ingest raw sensor events using streaming Auto Loader
        CREATE OR REFRESH STREAMING TABLE bronze_sensor_events
        CLUSTER BY (sensor_id)
        AS SELECT
          *,
          current_timestamp() AS _ingested_at,
          _metadata.file_path AS _source_file
        FROM STREAM read_files(
          '/Volumes/main/skill_test/raw_data/sensor_events/',
          format => 'json',
          schemaHints => 'sensor_id STRING, temperature DOUBLE, humidity DOUBLE, event_timestamp TIMESTAMP'
        );

        -- Silver: 5-minute tumbling window aggregations
        CREATE OR REFRESH STREAMING TABLE silver_sensor_5min
        CLUSTER BY (sensor_id)
        AS SELECT
          sensor_id,
          window(event_timestamp, '5 minutes').start AS window_start,
          window(event_timestamp, '5 minutes').end AS window_end,
          AVG(temperature) AS avg_temperature,
          MIN(temperature) AS min_temperature,
          MAX(temperature) AS max_temperature,
          AVG(humidity) AS avg_humidity,
          COUNT(*) AS event_count
        FROM STREAM bronze_sensor_events
        GROUP BY sensor_id, window(event_timestamp, '5 minutes');
        ```

        This pipeline:
        - Ingests raw sensor data incrementally using Auto Loader with STREAM wrapper
        - Uses tumbling windows (non-overlapping 5-minute intervals)
        - Aggregates temperature and humidity metrics per sensor per window
        - Uses event timestamp (not processing time) for accurate windowing
        - Uses CLUSTER BY for optimal query performance
      execution_success: true
    expectations:
      expected_facts:
        - "STREAMING TABLE"
        - "window"
        - "5 minutes"
        - "AVG"
        - "STREAM read_files"
      expected_patterns:
        - pattern: "CREATE OR REFRESH STREAMING TABLE"
          min_count: 2
          description: "streaming tables for bronze and silver"
        - pattern: "FROM STREAM read_files"
          min_count: 1
          description: "streaming source with STREAM wrapper (required for streaming tables)"
        - pattern: "window\\s*\\(.*,\\s*'5 minutes'\\)"
          min_count: 1
          description: "tumbling window function"
        - pattern: "GROUP BY.*window\\s*\\("
          min_count: 1
          description: "grouped by window"
        - pattern: "FROM STREAM bronze"
          min_count: 1
          description: "downstream streaming source reference"
      guidelines:
        - "Must use STREAM wrapper around read_files() for streaming ingestion"
        - "Must use tumbling windows for time-based aggregation"
        - "Should use event timestamp, not processing timestamp"
        - "Should aggregate multiple metrics per window"
    metadata:
      category: "happy_path"
      difficulty: "medium"
      source: "execution_verified"
      tags: ["streaming", "windowing", "aggregation", "iot", "execution_tested"]
      execution_verified:
        pipeline_id: "8208e047-476b-4fe1-9e46-ef472b239aa1"
        update_id: "21ef9b2f-e98b-4ed8-8381-8b48edd917a1"
        verified_date: "2026-01-23"
        tables_created:
          - name: "main.skill_test.bronze_sensor_events"
            rows: 100
          - name: "main.skill_test.silver_sensor_5min"
            rows: 54
        learnings:
          - "read_files() alone returns batch query; must wrap with STREAM for streaming tables"
          - "Error without STREAM: CREATE_APPEND_ONCE_FLOW_FROM_BATCH_QUERY_NOT_ALLOWED"

  - id: "sdp_bronze_python_001"
    inputs:
      prompt: "Create a bronze ingestion pipeline using Python only (no SQL) for JSON files in /Volumes/raw/order"
    outputs:
      response: |
        Here's a bronze ingestion pipeline using Python with the modern `pyspark.pipelines` API:

        ```python
        from pyspark import pipelines as dp
        from pyspark.sql.functions import current_timestamp, col

        @dp.table(
            name="bronze_orders",
            comment="Raw orders ingested from JSON files",
            cluster_by=["order_date"]
        )
        def bronze_orders():
            return (
                spark.readStream
                .format("cloudFiles")
                .option("cloudFiles.format", "json")
                .option("cloudFiles.inferColumnTypes", "true")
                .load("/Volumes/raw/order")
                .withColumn("_ingested_at", current_timestamp())
                .withColumn("_source_file", col("_metadata.file_path"))
            )
        ```

        This pipeline:
        - Uses the modern `pyspark.pipelines` (`dp`) API (recommended for 2025)
        - Uses Auto Loader (`cloudFiles`) for incremental file ingestion
        - Adds metadata columns for lineage tracking (`_ingested_at`, `_source_file`)
        - Uses Liquid Clustering with `cluster_by` instead of partitioning
      execution_success: null
    expectations:
      expected_facts:
        - "pyspark.pipelines"
        - "@dp.table"
        - "cloudFiles"
        - "_ingested_at"
      expected_patterns:
        - pattern: "from pyspark import pipelines as dp"
          min_count: 1
          description: "modern dp import"
        - pattern: "@dp\\.table\\("
          min_count: 1
          description: "dp.table decorator"
        - pattern: "cluster_by"
          min_count: 1
          description: "liquid clustering"
        - pattern: "\\.format\\([\"']cloudFiles[\"']\\)"
          min_count: 1
          description: "Auto Loader format"
      guidelines:
        - "Must use modern pyspark.pipelines API, not legacy dlt"
        - "Must use Python only, no SQL"
        - "Should include metadata columns for lineage"
    metadata:
      category: "happy_path"
      difficulty: "easy"
      source: "interactive"
      tags: ["bronze", "ingestion", "python", "autoloader"]

  - id: "sdp_cdc_scd2_python_001"
    inputs:
      prompt: "Implement auto_cdc using Python with SCD Type 2 for tracking customer dimension changes"
    outputs:
      response: |
        Here's an AUTO CDC implementation with SCD Type 2 using Python and the modern `pyspark.pipelines` API:

        ```python
        from pyspark import pipelines as dp
        from pyspark.sql.functions import col, current_timestamp

        # Bronze: Ingest raw CDC events from source
        @dp.table(
            name="bronze_customers_cdc",
            comment="Raw CDC events from customer source system",
            cluster_by=["customer_id"]
        )
        def bronze_customers_cdc():
            return (
                spark.readStream
                .format("cloudFiles")
                .option("cloudFiles.format", "json")
                .option("cloudFiles.inferColumnTypes", "true")
                .load("/Volumes/raw/customers_cdc")
                .withColumn("_ingested_at", current_timestamp())
            )

        # Silver: Apply SCD Type 2 using AUTO CDC
        dp.create_streaming_table(
            name="silver_customers_history",
            comment="Customer dimension with SCD Type 2 history tracking",
            cluster_by=["customer_id"]
        )

        dp.create_auto_cdc_flow(
            target="silver_customers_history",
            source="bronze_customers_cdc",
            keys=["customer_id"],
            sequence_by=col("event_timestamp"),
            stored_as_scd_type="2",
            track_history_column_list=["customer_name", "email", "phone", "address"]
        )
        ```

        This pipeline:
        - Uses the modern `pyspark.pipelines` (`dp`) API
        - Ingests CDC events from source using Auto Loader
        - Applies SCD Type 2 with automatic `__START_AT` and `__END_AT` columns
        - Tracks history on specified columns (changes to tracked columns create new versions)
        - Uses `create_auto_cdc_flow()` for CDC processing with SCD Type 2

        Query the current state with: `SELECT * FROM silver_customers_history WHERE __END_AT IS NULL`
      execution_success: null
    expectations:
      expected_facts:
        - "pyspark.pipelines"
        - "create_auto_cdc_flow"
        - "SCD Type 2"
        - "track_history"
      expected_patterns:
        - pattern: "from pyspark import pipelines as dp"
          min_count: 1
          description: "modern dp import"
        - pattern: "dp\\.create_auto_cdc_flow\\("
          min_count: 1
          description: "auto_cdc function for CDC"
        - pattern: "stored_as_scd_type.*2"
          min_count: 1
          description: "SCD Type 2 specification"
        - pattern: "dp\\.create_streaming_table\\("
          min_count: 1
          description: "streaming table creation"
        - pattern: "track_history_column_list"
          min_count: 1
          description: "history tracking columns"
      guidelines:
        - "Must use modern pyspark.pipelines API, not legacy dlt"
        - "Must use Python only, no SQL"
        - "Must use create_auto_cdc_flow for CDC operations"
        - "Must implement SCD Type 2"
        - "Should track history on specified columns"
    metadata:
      category: "happy_path"
      difficulty: "hard"
      source: "interactive"
      tags: ["cdc", "scd2", "python", "auto_cdc", "dimensions"]
