# Databricks Builder App - Environment Configuration
# ==================================================
#
# For LOCAL DEVELOPMENT: Copy this file to .env.local and fill in your values
# For DEPLOYMENT: Copy app.yaml.example to app.yaml and configure there
#
# This file is for local development only. When deploying to Databricks Apps,
# use app.yaml for environment configuration instead.

# =============================================================================
# Databricks Configuration (Local Development)
# =============================================================================
# Workspace URL and personal access token
# In production (Databricks Apps), authentication is handled automatically via
# the service principal's OAuth credentials - no token needed.
DATABRICKS_HOST=https://your-workspace.cloud.databricks.com
DATABRICKS_TOKEN=dapi...

# =============================================================================
# Database Configuration (Lakebase)
# =============================================================================
# Choose ONE of the following options:

# Option 1: Dynamic OAuth (recommended for Databricks Apps deployment)
# Uses Databricks SDK to generate OAuth tokens automatically
LAKEBASE_INSTANCE_NAME=your-lakebase-instance
LAKEBASE_DATABASE_NAME=databricks_postgres

# Option 2: Static connection URL (for local development)
# LAKEBASE_PG_URL=postgresql://user:password@host:5432/database?sslmode=require

# =============================================================================
# LLM Provider Configuration
# =============================================================================
# Choose your LLM provider: DATABRICKS (default) or AZURE
LLM_PROVIDER=DATABRICKS

# Databricks Foundation Models (adjust to models available in your workspace)
# Examples: databricks-meta-llama-3-3-70b-instruct, databricks-claude-sonnet-4
DATABRICKS_MODEL=databricks-meta-llama-3-3-70b-instruct
DATABRICKS_MODEL_MINI=databricks-gemini-3-flash

# Azure OpenAI (uncomment if using Azure instead)
# LLM_PROVIDER=AZURE
# AZURE_OPENAI_API_KEY=your-api-key
# AZURE_OPENAI_ENDPOINT=https://your-resource.cognitiveservices.azure.com/
# AZURE_OPENAI_API_VERSION=2024-08-01-preview
# AZURE_OPENAI_DEPLOYMENT=gpt-4o
# AZURE_OPENAI_DEPLOYMENT_MINI=gpt-4o-mini

# =============================================================================
# Skills Configuration
# =============================================================================
# Skills to include (comma-separated list of skill folder names)
ENABLED_SKILLS=databricks-python-sdk,spark-declarative-pipelines,synthetic-data-generation,unstructured-pdf-generation,agent-bricks

# Optional: Add additional skills
# ENABLED_SKILLS=databricks-python-sdk,spark-declarative-pipelines,synthetic-data-generation,unstructured-pdf-generation,agent-bricks

# Test mode: only enable Skill tool (useful for debugging)
SKILLS_ONLY_MODE=false

# =============================================================================
# Application Settings
# =============================================================================
# Projects directory (where Claude Code agent will work)
PROJECTS_BASE_DIR=./projects

# Environment (development or production)
ENV=development

# Claude SDK stream timeout in milliseconds (default: 1 hour)
# Increase if you have very long-running operations
CLAUDE_CODE_STREAM_CLOSE_TIMEOUT=3600000

# Anthropic API key (optional - uses Databricks model serving by default)
# ANTHROPIC_API_KEY=sk-ant-...

# =============================================================================
# DEPLOYMENT TO DATABRICKS APPS
# =============================================================================
#
# To deploy this app to Databricks Apps:
#
# 1. Prerequisites:
#    - Databricks CLI installed and configured (databricks auth login)
#    - A Lakebase instance created in your workspace
#
# 2. Create the app:
#    databricks apps create <your-app-name>
#
# 3. Configure app.yaml:
#    cp app.yaml.example app.yaml
#    # Edit app.yaml with your Lakebase instance name and LLM settings
#
# 4. Add Lakebase as an app resource:
#    databricks apps add-resource <your-app-name> \
#      --resource-type database \
#      --resource-name lakebase \
#      --database-instance <your-lakebase-instance-name>
#
# 5. Grant table permissions to the app's service principal:
#    (Run this SQL in a notebook or SQL editor after first deployment)
#    
#    GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public 
#      TO `<service-principal-id>`;
#    GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA public 
#      TO `<service-principal-id>`;
#    ALTER DEFAULT PRIVILEGES IN SCHEMA public 
#      GRANT ALL ON TABLES TO `<service-principal-id>`;
#
# 6. Deploy:
#    ./scripts/deploy.sh <your-app-name>
#
# For more details, see the README.md
