# Project Initialization with databricks pipelines init

## Overview

The `databricks pipelines init` command scaffolds a complete Databricks Asset Bundle project for Lakeflow Spark Declarative Pipelines, providing a production-ready structure with multi-environment support, pipeline configuration, and sample transformation files.

**Benefits of Asset Bundles:**
- Multi-environment deployments (dev/staging/prod)
- Infrastructure as code with `databricks.yml`
- Built-in CI/CD integration
- Version control for pipeline configuration
- Automated deployment workflows

---

## Command Reference

### Interactive Mode

```bash
databricks pipelines init --output-dir .
```

**Interactive Prompts:**

1. **Project name** (default: `my_pipeline_project`)
   - Used for bundle name, pipeline name, and folder structure
   - Example: `customer_orders_pipeline`

2. **Initial catalog** (Unity Catalog name)
   - Must be an existing Unity Catalog catalog
   - Example: `main`, `prod_catalog`, `dev_catalog`

3. **Use personal schema for each user?** (yes/no)
   - `yes`: Schema is `${workspace.current_user.short_name}` (recommended for dev)
   - `no`: Schema is fixed value (recommended for prod)

4. **Initial language** (python/sql)
   - Determines whether sample transformation files are `.py` or `.sql`
   - Both SQL and Python can be used in the same project

### Non-Interactive Mode

```bash
databricks pipelines init \
  --output-dir . \
  --config-file init-config.json
```

**Example init-config.json:**
```json
{
  "project_name": "customer_pipeline",
  "initial_catalog": "prod_catalog",
  "use_personal_schema": "no",
  "initial_language": "sql"
}
```

**Use non-interactive mode for:**
- Automated project generation scripts
- Templating workflows
- CI/CD pipeline initialization
- Batch project creation

---

## Generated Structure

### SQL Project

```
project_root/
├── databricks.yml                           # Bundle configuration
├── resources/
│   ├── customer_pipeline_etl.pipeline.yml  # Pipeline resource definition
│   └── sample_job.job.yml                  # Optional scheduled job
├── README.md                                # Auto-generated documentation
└── src/
    └── customer_pipeline_etl/
        ├── README.md                        # ETL folder documentation
        ├── explorations/
        │   └── sample_exploration.ipynb     # Notebook for ad-hoc queries
        └── transformations/
            ├── sample_trips_customer_pipeline.sql
            └── sample_zones_customer_pipeline.sql
```

### Python Project

```
project_root/
├── databricks.yml                           # Bundle configuration
├── pyproject.toml                           # Python dependencies
├── resources/
│   ├── customer_pipeline_etl.pipeline.yml  # Pipeline resource definition
│   └── sample_job.job.yml                  # Optional scheduled job
├── README.md                                # Auto-generated documentation
└── src/
    └── customer_pipeline_etl/
        ├── README.md                        # ETL folder documentation
        ├── explorations/
        │   └── sample_exploration.ipynb     # Notebook for ad-hoc queries
        └── transformations/
            ├── sample_trips_customer_pipeline.py
            └── sample_zones_customer_pipeline.py
```

**Key Differences:**
- Python projects include `pyproject.toml` for dependency management
- Transformation files use `.py` extension with `@dp.table` decorators
- Both use the same bundle structure and deployment process

---

## Customization Workflow

### 1. Replace Sample Files

The generated project includes sample transformation files that can be replaced:

```bash
cd src/customer_pipeline_etl/transformations/

# Remove sample files
rm sample_*.sql  # or sample_*.py for Python

# Add your transformation files
touch bronze_orders.sql
touch silver_cleaned_orders.sql
touch gold_daily_summary.sql
```

### 2. Update databricks.yml

Configure target environments in the root `databricks.yml`:

```yaml
bundle:
  name: customer_pipeline
  uuid: <auto-generated>

include:
  - resources/*.yml
  - resources/*/*.yml

variables:
  catalog:
    description: The catalog to use
  schema:
    description: The schema to use

targets:
  dev:
    mode: development
    default: true
    workspace:
      host: https://your-workspace.cloud.databricks.com
    variables:
      catalog: dev_catalog
      schema: ${workspace.current_user.short_name}

  prod:
    mode: production
    workspace:
      host: https://your-workspace.cloud.databricks.com
      root_path: /Workspace/Users/you@example.com/.bundle/${bundle.name}/${bundle.target}
    variables:
      catalog: prod_catalog
      schema: production
    permissions:
      - user_name: you@example.com
        level: CAN_MANAGE
```

**Key Configuration Options:**
- `mode: development` - Prefixes resources with `[dev username]`, pauses schedules
- `mode: production` - No prefix, enables schedules
- `variables` - Parameterize catalog and schema for different environments
- `permissions` - Control access to deployed resources

### 3. Customize Pipeline Configuration

Edit `resources/*_etl.pipeline.yml` to adjust pipeline settings:

```yaml
resources:
  pipelines:
    customer_pipeline_etl:
      name: customer_pipeline_etl
      catalog: ${var.catalog}
      schema: ${var.schema}
      serverless: true
      root_path: "../src/customer_pipeline_etl"
      libraries:
        - glob:
            include: ../src/customer_pipeline_etl/transformations/**
      environment:
        dependencies:
          - --editable ${workspace.file_path}
      # Optional: Add development mode for faster iteration
      # development: true
      # Optional: Add continuous mode for always-running pipeline
      # continuous: false
```

### 4. Deploy to Workspace

```bash
# Validate configuration
databricks bundle validate

# Deploy to dev (default target)
databricks bundle deploy

# Deploy to prod
databricks bundle deploy --target prod

# Deploy and run immediately
databricks bundle run customer_pipeline_etl
```

**Deployment Process:**
1. Uploads files to workspace
2. Creates/updates pipeline resource
3. Applies target-specific configuration
4. Sets permissions (if configured)

### 5. Run Pipeline

```bash
# Run via bundle (uses default target)
databricks bundle run customer_pipeline_etl

# Run specific target
databricks bundle run customer_pipeline_etl --target prod

# Or use Pipeline API directly
databricks pipelines start-update --pipeline-id <id>
```

---

## Language Detection (for Claude)

When a user requests a new Lakeflow pipeline, Claude should detect the appropriate language from keywords in the prompt.

### CRITICAL: Explicit Language Requests

**If the user explicitly mentions a language, use it without asking:**

| User Says | Action |
|-----------|--------|
| "Python pipeline", "Python SDP", "use Python" | **Use Python immediately** |
| "SQL pipeline", "SQL files", "use SQL" | **Use SQL immediately** |
| "Python Spark Declarative Pipeline" | **Use Python immediately** |

**DO NOT ask for clarification when the user explicitly states a language.** This is the most common mistake - ignoring an explicit language request.

### SQL Indicators (Default Choice When Ambiguous)

**Keywords:**
- "sql files", ".sql"
- "simple", "basic", "straightforward"
- "aggregations", "joins", "transformations"
- "materialized view", "CREATE OR REFRESH"
- "SELECT", "GROUP BY", "WHERE"

**Context:**
- User mentions only data transformations without complex logic
- Request focuses on filtering, joining, aggregating data
- No mention of custom functions or external integrations
- **No explicit mention of "Python"**

**Default Behavior**: Prefer SQL only when ambiguous AND no Python indicators present

### Python Indicators

**Keywords:**
- "Python", "python files", ".py", "@dp.table"
- "UDF", "user-defined function", "custom function"
- "complex logic", "complex transformations"
- "ML", "machine learning", "inference", "model"
- "API", "external API", "REST", "HTTP"
- "pandas", "numpy", "pyspark"
- "decorator", "pyspark.pipelines"

**Context:**
- User needs custom data processing beyond SQL capabilities
- Request mentions integrating with external services
- Task requires ML model inference or scoring
- Dynamic schema or path generation needed

### Ambiguous Cases (Ask User)

**Only ask when ALL conditions are met:**
- User did NOT explicitly mention "Python" or "SQL"
- Mixed signals present (some SQL keywords, some Python keywords)
- OR no clear indicators either way

**Response:**
```
I can create this pipeline using either SQL or Python:

- **SQL**: Best for transformations, aggregations, joins (simpler, faster to develop)
- **Python**: Best for custom logic, UDFs, ML inference, external APIs

Which would you prefer?
```

---

## Medallion Architecture

For bronze/silver/gold organization, Asset Bundles support two approaches. Both work with the `transformations/**` glob pattern in pipeline configuration.

### Option 1: Flat Structure with Naming (Template Default, SQL Example)

```
transformations/
├── bronze_raw_orders.sql          # Raw data ingestion
├── bronze_raw_events.sql
├── bronze_raw_customers.sql
├── silver_cleaned_orders.sql      # Cleaned and validated
├── silver_joined_data.sql
├── silver_customer_profiles.sql
├── gold_daily_metrics.sql         # Business aggregations
├── gold_customer_summary.sql
└── gold_revenue_analysis.sql
```

**Advantages:**
- Matches the official `databricks pipelines init` template structure
- All files visible at one level
- Simple file listing and discovery
- Clear naming provides logical organization

### Option 2: Subdirectories by Layer, SQL Example

```
transformations/
├── bronze/
│   ├── raw_orders.sql
│   ├── raw_events.sql
│   └── raw_customers.sql
├── silver/
│   ├── cleaned_orders.sql
│   ├── joined_data.sql
│   └── customer_profiles.sql
└── gold/
    ├── daily_metrics.sql
    ├── customer_summary.sql
    └── revenue_analysis.sql
```

**Advantages:**
- Physical separation of layers
- Familiar structure for teams using manual workflow
- Easier to navigate large projects with many files
- Works with `transformations/**` glob pattern

**Both approaches are technically valid** - the `**` in the glob pattern matches files recursively. Choose based on team preference and project size.

### Example Bronze Layer (SQL)

```sql
-- File: bronze_raw_orders.sql
CREATE OR REFRESH STREAMING TABLE bronze_raw_orders
CLUSTER BY (order_date)
COMMENT "Raw order data ingested from cloud storage"
AS
SELECT
  *,
  current_timestamp() AS _ingested_at,
  _metadata.file_path AS _source_file
FROM read_files(
  '/Volumes/main/raw_data/orders/',
  format => 'json',
  schemaHints => 'order_id STRING, customer_id STRING, amount DECIMAL(10,2), order_date DATE'
);
```

### Example Silver Layer (SQL)

```sql
-- File: silver_cleaned_orders.sql
CREATE OR REFRESH MATERIALIZED VIEW silver_cleaned_orders
CLUSTER BY (order_date)
COMMENT "Cleaned and validated orders with customer enrichment"
AS
SELECT
  o.order_id,
  o.customer_id,
  o.amount,
  o.order_date,
  c.customer_name,
  c.customer_segment
FROM LIVE.bronze_raw_orders o
INNER JOIN LIVE.bronze_raw_customers c
  ON o.customer_id = c.customer_id
WHERE o.amount > 0  -- Remove invalid orders
  AND o.order_date >= '2020-01-01';
```

### Example Gold Layer (SQL)

```sql
-- File: gold_daily_metrics.sql
CREATE OR REFRESH MATERIALIZED VIEW gold_daily_metrics
CLUSTER BY (metric_date)
COMMENT "Daily business metrics for reporting"
AS
SELECT
  order_date AS metric_date,
  COUNT(DISTINCT customer_id) AS unique_customers,
  COUNT(*) AS total_orders,
  SUM(amount) AS total_revenue,
  AVG(amount) AS avg_order_value
FROM LIVE.silver_cleaned_orders
GROUP BY order_date;
```

---

## Migration from Manual Structure

### Option 1: Migrate to Bundle (Recommended)

If you have an existing manual structure with separate folders:

**Old Structure:**
```
my_pipeline/
├── bronze/
│   ├── orders.sql
│   └── events.sql
├── silver/
│   ├── cleaned.sql
│   └── joined.sql
└── gold/
    └── summary.sql
```

---

## Python Project: Dependency Management

### Using pyproject.toml

Python bundle projects include `pyproject.toml` for dependency management:

```toml
[project]
name = "customer_pipeline"
version = "0.0.1"
dependencies = [
    # Add your runtime dependencies here
    # "pandas>=2.0.0",
    # "scikit-learn==1.3.0",
]

[project.optional-dependencies]
dev = [
    "pytest",
    "ruff",
    "databricks-dlt",
    "databricks-connect>=15.4,<15.5",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.ruff]
line-length = 120
```

### Adding Dependencies

1. **Runtime dependencies** (available during pipeline execution):
   ```toml
   dependencies = [
       "pandas>=2.0.0",
       "requests==2.31.0",
   ]
   ```

2. **Development dependencies** (local development only):
   ```toml
   [project.optional-dependencies]
   dev = [
       "pytest",
       "ruff",
   ]
   ```

3. **Deploy with dependencies**:
   ```bash
   databricks bundle deploy
   ```

   The pipeline configuration includes:
   ```yaml
   environment:
     dependencies:
       - --editable ${workspace.file_path}
   ```

   This installs your package and dependencies on serverless compute.

---

## Troubleshooting

### "Command not found: databricks"

**Problem**: Databricks CLI not installed

**Solution**:
```bash
pip install databricks-cli
# or
pip install --upgrade databricks-cli
```

### "Invalid catalog name"

**Problem**: Specified catalog doesn't exist in Unity Catalog

**Solution**:
```bash
# List available catalogs
databricks catalogs list

# Create catalog if needed
databricks catalogs create --name my_catalog
```

### "Language option not recognized"

**Problem**: Incorrect language parameter format

**Solution**: Use lowercase values:
- Correct: `"initial_language": "sql"` or `"initial_language": "python"`
- Incorrect: `"initial_language": "SQL"` or `"initial_language": "Python"`

### "Files not found during deployment"

**Problem**: Pipeline configuration glob pattern doesn't match your files

**Solution**: Check `resources/*_etl.pipeline.yml`:
```yaml
libraries:
  - glob:
      include: ../src/my_pipeline_etl/transformations/**
      # Make sure this path matches your file locations
```

### "Pipeline deployment failed: Authentication error"

**Problem**: Databricks authentication not configured

**Solution**:
```bash
# Configure authentication
databricks configure --host https://your-workspace.cloud.databricks.com

# Or set environment variables
export DATABRICKS_HOST="https://your-workspace.cloud.databricks.com"
export DATABRICKS_TOKEN="your-personal-access-token"
```

### "Bundle validation failed: Invalid schema"

**Problem**: databricks.yml has syntax errors

**Solution**:
```bash
# Validate configuration
databricks bundle validate

# Check YAML syntax
# Ensure proper indentation (use spaces, not tabs)
# Verify required fields are present
```

### Files Deploy But Pipeline Doesn't Update

**Problem**: Pipeline configuration not refreshed

**Solution**:
```bash
# Force full deployment
databricks bundle deploy --force

# Or delete and recreate
databricks bundle destroy
databricks bundle deploy
```

---

## Advanced Configuration

For advanced pipeline configuration options beyond the bundle initialization:

- **Development mode**: Faster iteration, allows table deletion
- **Continuous mode**: Always-running pipelines for streaming
- **Custom notifications**: Email or webhook alerts
- **Non-serverless clusters**: When serverless limitations apply

See [7-advanced-configuration.md](7-advanced-configuration.md) for detailed examples.

---

## Working with Multiple Environments

### Development Workflow

```bash
# Work in dev environment (default)
databricks bundle deploy
databricks bundle run my_pipeline_etl

# Resources are prefixed: [dev username]my_pipeline_etl
# Tables written to: dev_catalog.username.table_name
```

### Production Deployment

```bash
# Deploy to production
databricks bundle deploy --target prod

# Run in production
databricks bundle run my_pipeline_etl --target prod

# No prefix: my_pipeline_etl
# Tables written to: prod_catalog.production.table_name
```

### Environment-Specific Configuration

**databricks.yml:**
```yaml
targets:
  dev:
    variables:
      catalog: dev_catalog
      schema: ${workspace.current_user.short_name}

  prod:
    variables:
      catalog: prod_catalog
      schema: production
```

**Pipeline uses variables:**
```yaml
resources:
  pipelines:
    my_pipeline_etl:
      catalog: ${var.catalog}  # dev_catalog or prod_catalog
      schema: ${var.schema}    # username or production
```

---

## Best Practices

### Project Organization

1. **Use descriptive file names**: `bronze_orders_raw.sql` not just `orders.sql`
2. **Choose structure approach**:
   - **Flat with prefixes**: `bronze_*`, `silver_*`, `gold_*` (template default)
   - **Subdirectories**: `bronze/`, `silver/`, `gold/` folders (also valid)
   - Both work with `transformations/**` glob pattern
3. **One table per file**: Each file defines a single table or view
4. **Be consistent**: Pick one approach and use it throughout the project

### Configuration Management

1. **Use variables**: Parameterize catalog and schema names
2. **Separate environments**: Define dev/staging/prod targets
3. **Version control**: Track `databricks.yml` and pipeline configs in git
4. **Sensitive data**: Use secrets, not hardcoded values

### Development Workflow

1. **Start with dev**: Always test in development environment first
2. **Validate locally**: Run `databricks bundle validate` before deploy
3. **Incremental changes**: Deploy and test small changes frequently
4. **Use explorations**: Ad-hoc notebooks for data exploration

### Deployment Strategy

1. **CI/CD integration**: Automate deployments with GitHub Actions, GitLab CI
2. **Approval gates**: Require approval for production deployments
3. **Rollback plan**: Keep previous bundle versions for quick rollback
4. **Monitor pipelines**: Set up notifications for failures

---

## References

- **[SKILL.md](SKILL.md)** - Main development workflow and MCP tools
- **[Databricks Asset Bundles Documentation](https://docs.databricks.com/dev-tools/bundles/)** - Official bundle reference
- **[Pipeline Configuration Reference](https://docs.databricks.com/aws/en/ldp/configure-pipeline)** - Pipeline settings
- **[Databricks CLI Reference](https://docs.databricks.com/dev-tools/cli/)** - CLI commands and options
- **[1-ingestion-patterns.md](1-ingestion-patterns.md)** - Data ingestion patterns
- **[2-streaming-patterns.md](2-streaming-patterns.md)** - Streaming transformations
- **[7-advanced-configuration.md](7-advanced-configuration.md)** - Advanced pipeline settings
